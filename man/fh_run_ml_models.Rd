% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fh_run_ml_models.R
\name{fh_run_ml_models}
\alias{fh_run_ml_models}
\title{Run all 27 ML models in one shot (orchestrator wrapper)}
\usage{
fh_run_ml_models(
  train_exp,
  train_labels,
  test_exp,
  labels_list,
  lassogene,
  com_genes,
  all_labels,
  fold = 10,
  cutoff = c(0.25, 0.5, 0.75),
  knumber = c(1, 3, 5),
  alpha_all = seq(0.1, 0.9, 0.1),
  kernel_all = c("linear", "polynomial", "radial"),
  nround = 100,
  max_depth = 6,
  eta = 0.5,
  auto_th_method = "youden",
  cores = NULL,
  which_models = "all"
)
}
\arguments{
\item{train_exp}{matrix/data.frame
Training expression matrix/data frame (rows=samples, cols=features).}

\item{train_labels}{vector/factor
Training labels (0/1 or binary factor).}

\item{test_exp}{list
List of external datasets, e.g., \code{list(B, C, D)}.}

\item{labels_list}{list
List of label data.frames aligned with datasets (\verb{[[1]]} for A/Train, \verb{[[2]]} for B/Val).}

\item{lassogene}{character NULL
Vector of LASSO-selected feature names, passed to \emph{lasso} variants.}

\item{com_genes}{character NULL
Shared feature set for Random Forest (if required by your rf flow).}

\item{all_labels}{any NULL
If \code{fh_logistic()} needs \code{all_labels}, it will be passed through.}

\item{fold}{integer = 10
Number of folds (default 10) for functions that need it.}

\item{cutoff}{numeric = c(0.25,0.5,0.75)
Base cutoff grid to pass into models that accept it.}

\item{knumber}{integer = c(1,3,5)
Vector of K values for KNN.}

\item{alpha_all}{numeric = seq(0.1,0.9,0.1)
Alpha grid for Elastic Net.}

\item{kernel_all}{character = c("linear","polynomial","radial")
Kernels for SVM.}

\item{nround}{integer = 100
Default number of boosting rounds for XGBoost.}

\item{max_depth}{integer = 6
Maximum depth of XGBoost trees (default 6).}

\item{eta}{numeric = 0.5
Learning rate (eta) for XGBoost (default 0.5).}

\item{auto_th_method}{character = "youden"
Auto threshold method for functions exposing it.}

\item{cores}{integer(NULL)
Parallel cores. If \code{NULL}, uses \code{max(1, detectCores()-1)}. Passed to GBM CV-based functions that need it.}

\item{which_models}{character = "all"
Which model runners to execute. \code{"all"} runs all 27, or pass a subset.}
}
\value{
list
Named list (size ≤ 27) of each model’s return value. Errors are caught and returned as \code{NULL} with a message.
}
\description{
This function orchestrates and sequentially calls your existing 27
model runners (listed below), collecting their return values with tryCatch
and returning a named list. It \strong{does not} modify your existing functions or
helpers—just a thin wrapper for “batch run + aggregate results”.
}
\section{Model runners included}{

\itemize{
\item \code{fh_logistic()}, \code{fh_lda()}, \code{fh_lasso_lda()}, \code{fh_qda()}, \code{fh_lasso_qda()},
\code{fh_knn()}, \code{fh_lasso_knn()}, \code{fh_tree()}, \code{fh_lasso_tree()}, \code{fh_rf()},
\code{fh_lasso_rf()}, \code{fh_xgboost()}, \code{fh_lasso_xgboost_default()},
\code{fh_lasso_xgboost_best()}, \code{fh_ridge_glmnet()}, \code{fh_lasso()},
\code{fh_elastic_net()}, \code{fh_svm()}, \code{fh_svm_best()}, \code{fh_lasso_svm_best()},
\code{fh_gbm_default()}, \code{fh_gbm_cv_best()}, \code{fh_gbm_lasso_default()},
\code{fh_gbm_lasso_best()}, \code{fh_stepwise_lr()}, \code{fh_naive_bayes()},
\code{fh_naive_bayes_lasso()}
}
}

\examples{
\dontrun{
res_all <- fh_run_ml_models(
  train_exp    = train_exp,
  train_labels = train_labels,
  test_exp     = test_exp,
  labels_list  = labels_list,
  lassogene    = lassogene,
  com_genes    = com_genes,
  all_labels   = all_labels,
  fold         = 10,
  cutoff       = c(0.25, 0.5, 0.75),
  knumber      = c(1,3,5),
  alpha_all    = seq(0.1,0.9,0.1),
  kernel_all   = c("linear","polynomial","radial"),
  nround       = 100,
  max_depth    = 6,
  eta          = 0.5,
  auto_th_method = "youden",
  cores        = NULL,
  which_models = "all"
)
}
}
